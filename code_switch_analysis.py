# -*- coding: utf-8 -*-
"""code_switch_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10aEgzwThUpPD8cPhASvqOZ36Z3q5Xbz6
"""

! pip install pandas
! pip install praat-parselmouth librosa
! pip install torchaudio
! pip install transformers
! pip install pydub
! pip install textgrid
! pip install syllables

import glob
import os
import librosa
from google.colab import drive
import pandas as pd
import langid
import textgrid
from langdetect import detect
import re
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/dataverse_files/Spice_Data.csv')

df = df[df['language']=="Cantonese"]
print(len(df[df['code_switching'] == True]))
print(len(df[df['code_switching'] == False]))
words_df = pd.DataFrame(columns=['file', 'utterance', 'word', 'word_start', 'word_end'])
phones_df = pd.DataFrame(columns=['file', 'utterance', 'phone', 'phone_start', 'phone_end'])
df = df[df['language'] == "English"]
#Defining vowels for later analysis
vowels = ["ER0", "IH2", "EH1", "AE0", "UH1", "AY2", "AW2", "UW1", "OY2", "OY1", "AO0", "AH2", "ER1", "AW1",
          "OW0", "IY1", "IY2", "UW0", "AA1", "EY0", "AE1", "AA0", "OW1", "AW0", "AO1", "AO2", "IH0", "ER2",
          "UW2", "IY0", "AE2", "AH0", "AH1", "UH2", "EH2", "UH0", "EY1", "AY0", "AY1", "EH0", "EY2", "AA2",
          "OW2", "IH1"]

#Going through SpiCE Dataframe with utterances onsets and offsets
for index, row in df.iterrows():
  print(row['utterance'], row['file'])
  #Load in TextGrid for examination
  tg_file = '/content/drive/MyDrive/dataverse_files/english/'+ row['file'] + ".TextGrid"
  tg = textgrid.TextGrid.fromFile(tg_file)

  #Iterate through textgrids
  for tier in tg:
      if tier.name == "word":
        for interval in tier.intervals:
          if (interval.minTime > row['utterance_onset']) & (interval.minTime < row['utterance_offset']):
            pass
          else:
            continue
          word = interval.mark
          df_word = {'file': row['file'], 'utterance': row['utterance'], 'word': word,
                   'word_start': interval.minTime, 'word_end': interval.maxTime, 'cs_next': False}
          words_df = words_df.append(df_word, ignore_index = True)
      elif tier.name == "phone":
        for interval in tier.intervals:
          if interval.minTime >= row['utterance_onset'] and interval.minTime <= row['utterance_offset']:
            pass
          else:
            continue
          df_phone = {'file': row['file'], 'utterance': row['utterance'], 'phone': interval.mark,
                   'phone_start': interval.minTime, 'phone_end': interval.maxTime}
          phones_df = phones_df.append(df_phone, ignore_index = True)

print(phones_df)
print(words_df)
words_df = words_df.dropna()
phones_df = phones_df.dropna()
phones_df.to_csv('/content/drive/MyDrive/dataverse_files/phoneme_data.csv')
words_df.to_csv('/content/drive/MyDrive/dataverse_files/word_data.csv')

import pyphen
import syllables
analysis = ["ER0", "IH2", "EH1", "AE0", "UH1", "AY2", "AW2", "UW1", "OY2", "OY1", "AO0", "AH2", "ER1", "AW1",
          "OW0", "IY1", "IY2", "UW0", "AA1", "EY0", "AE1", "AA0", "OW1", "AW0", "AO1", "AO2", "IH0", "ER2",
          "UW2", "IY0", "AE2", "AH0", "AH1", "UH2", "EH2", "UH0", "EY1", "AY0", "AY1", "EH0", "EY2", "AA2",
          "OW2", "IH1", "T", "D", "K", "G", "P", "B"]

def in_range(n, start, end = 0):
  return start <= n <= end if end >= start else end <= n <= start

#So now we have two DataFrames with the Timestamps of each Phoneme and words, let combines them
words_df = pd.read_csv('/content/drive/MyDrive/dataverse_files/word_data.csv')
# print(len(words_df))
phones_df = pd.read_csv('/content/drive/MyDrive/dataverse_files/phoneme_data.csv')
words_df = words_df.dropna()
words_df = words_df.reset_index()
print(len(words_df))
for index, row in words_df.iterrows():
  # print(index)
  # print(len(words_df))
  if index == len(words_df) - 1:
    break
  else:
    next_word = words_df.at[index + 1, 'word']
    print(row['word'], next_word)
    if next_word == "<unk>":
      words_df.at[index, 'cs_next'] = True
  if row['word']  == "<unk>":
    words_df.drop(index, inplace=True)

words_df['syll/dur'] = ''
for index, row in words_df.iterrows():
  dic = pyphen.Pyphen(lang='en')
  # parsed = dic.inserted(row['word'])
  # print(parsed)
  count = syllables.estimate(row['word'])
  dur = row['word_end'] - row['word_start']
  # for x in parsed:
  #   if x == "-":
  #     count = count + 1

  print(count)
  words_df.at[index, 'syll/dur'] = count / dur

words_df.to_csv('/content/drive/MyDrive/dataverse_files/speech_rate2.csv')
# print(words_df[words_df['cs_next']== True])
phones_df['word'] = ''
phones_df['word_start'] = ''
phones_df['word_end'] = ''
phones_df['cs_next'] = ''
count = 0
phones_df = phones_df[phones_df['phone'].isin(analysis)]
cs_df = words_df[words_df['cs_next'] == True]
no_cs_df = words_df[words_df['cs_next'] == False]
no_cs_df = no_cs_df.sample(448)
df = pd.concat([no_cs_df, cs_df])
print(df[df['cs_next'] == True])
print(phones_df)
# cs_df = cs_df.sample(50)
for index, row in phones_df.iterrows():
  count = count + 1
  # print(str(count) + "/" + str(len(phones_df)))
  phone = row['phone']
  start = row['phone_start']
  filename = row['file']
  utterance = row['utterance']
  phone_index = index
  for word_index, word_row in df.iterrows():
    # print(str(count) + "/" + str(len(df)))
    if filename == word_row['file'] and in_range(start, word_row['word_start'], word_row['word_end']):
      print(word_row['cs_next'])
      phones_df.at[index, 'word_start'] = word_row['word_start']
      phones_df.at[index, 'word_end'] = word_row['word_end']
      phones_df.at[index, 'word'] = word_row['word']
      phones_df.at[index, 'cs_next'] = word_row['cs_next']
    else:
      pass

print(phones_df[phones_df['cs_next']== True])
phones_df.to_csv('/content/drive/MyDrive/dataverse_files/phoneme_and_word_data.csv')

import os
import numpy as np
from scipy.io import wavfile
import parselmouth
from parselmouth.praat import call
from IPython.display import Audio
import matplotlib.pyplot as plt
import pyphen

# dic = pyphen.Pyphen(lang='en')
# print dic.inserted('Rohit')


words_df['pitch'] = ''
words_df['pitch_change_next'] = ''

for index, row in words_df.iterrows():
  if index == len(words_df) - 1:
    break
  else:
    next_word = words_df.at[index + 1, 'word']
    # print(row['word'], next_word)
    if next_word == "<unk>":
      words_df.at[index, 'cs_next'] = True

def pitch_praat(x, start, end):
        f0min, f0max = 75, 300
        sound = parselmouth.Sound(x) # read the sound
        sound = sound.extract_part(from_time = start, to_time = end)
        pitch = sound.to_pitch_cc()
        f0 = pitch.selected_array['frequency']

        return f0

for index, row in words_df.iterrows():
  print(str(index) + "/" + str(len(words_df)))
  start = row['word_start']
  end = row['word_end']
  sound = "/content/drive/MyDrive/dataverse_files/english/" + row['file'] + ".wav"
  result = pitch_praat(sound, start, end)
  words_df.at[index, 'pitch'] = result

import os
import numpy as np
from scipy.io import wavfile
import parselmouth
from parselmouth.praat import call
from IPython.display import Audio
import matplotlib.pyplot as plt

phones_df = pd.read_csv('/content/drive/MyDrive/dataverse_files/phoneme_and_word_data.csv')
print(phones_df[phones_df['cs_next']== True])
print(phones_df.dropna())
phones_df = phones_df.dropna()
def find_middle(lst):
    if not lst:  # Check if the list is empty
        return "The list is empty."

    length = len(lst)  # Get the length of the list

    if length % 2 != 0:  # Check if the length is odd
        middle_index = length // 2
        return lst[middle_index]

    # If the length is even
    first_middle_index = length // 2 - 1
    second_middle_index = length // 2
    return ((lst[first_middle_index] + lst[second_middle_index]) // 2)

def formants_praat(x, start, end):
        f0min, f0max = 75, 300
        sound = parselmouth.Sound(x) # read the sound
        sound = sound.extract_part(from_time = start, to_time = end)
        pitch = sound.to_pitch()
        f0 = pitch.selected_array['frequency']
        formants = sound.to_formant_burg(time_step=0.010, maximum_formant=5000)

        f1_list, f2_list, f3_list, f4_list  = [], [], [], []
        for t in formants.ts():
            f1 = formants.get_value_at_time(1, t)
            f2 = formants.get_value_at_time(2, t)
            f3 = formants.get_value_at_time(3, t)
            f4 = formants.get_value_at_time(4, t)
            if np.isnan(f1): f1 = 0
            if np.isnan(f2): f2 = 0
            if np.isnan(f3): f3 = 0
            if np.isnan(f4): f4 = 0
            f1_list.append(f1)
            f2_list.append(f2)
            f3_list.append(f3)
            f4_list.append(f4)

        return f0, f1_list, f2_list, f3_list, f4_list

phones_analysis = phones_df[phones_df['phone'].isin(vowels)]
phones_analysis['F1'] = ""
phones_analysis['F2'] = ""
phones_analysis['F0'] = ''
print(phones_analysis)
for index, row in phones_analysis.iterrows():
  sound = "/content/drive/MyDrive/dataverse_files/english/" + row['file'] + ".wav"
  print(row['utterance'])
  start = row['phone_start']
  end = row['phone_end']
  f0, f1, f2, f3, f4 = formants_praat(sound, start, end)
  f1 = find_middle(f1)
  f2 = find_middle(f2)
  phones_analysis.at[index,'F1'] = f1
  phones_analysis.at[index,'F2'] = f2
  phones_analysis.at[index,'F0'] = f0


print(phones_analysis)
phones_analysis.to_csv('/content/drive/MyDrive/dataverse_files/phoneme_analysis_sample_data.csv')

phones_analysis = pd.read_csv('/content/drive/MyDrive/dataverse_files/phoneme_analysis_sample_data.csv')

print(phones_analysis[phones_analysis['cs_next'] == True])

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.ticker as ticker
from matplotlib.ticker import ScalarFormatter

def vowelplot (vowelcsv, color=None, F1="F1", F2="F2", vowel="phone", title="Vowel Plot", unit="Hz", logscale=True):

    #Set some parameters for the chart itself
    sns.set(style='ticks', context='notebook')
    plt.figure(figsize=(6,6))

    # If there's an argument for color, determine whether it's likely to be categorical
    ## If it's a string (text), use a categorical color palette
    ## If it's a number, use a sequential color palette
    if color != None:
        if type(vowelcsv[color].iloc[0])==str:
            pal = "husl"
        else:
            pal = "viridis"

        pl = sns.scatterplot(x = F2,
                             y = F1,
                             hue = color,
                             data = vowelcsv,
                             palette = pal)

    # If no color argument is given, don't specify hue, and no palette needed
    else:
        pl = sns.scatterplot(x = F2,
                             y = F1,
                             data = vowelcsv)


    #Invert axes to correlate with articulatory space!
    pl.invert_yaxis()
    pl.invert_xaxis()

    #Add unit to the axis labels
    F1name = str("F1 ("+unit+")")
    F2name = str("F2 ("+unit+")")
    laby = plt.ylabel(F1name)
    labx = plt.xlabel(F2name)

    if logscale == True:
        pl.loglog()
        pl.yaxis.set_major_formatter(ticker.ScalarFormatter())
        pl.yaxis.set_minor_formatter(ticker.ScalarFormatter())
        pl.xaxis.set_major_formatter(ticker.ScalarFormatter())
        pl.xaxis.set_minor_formatter(ticker.ScalarFormatter())

    # Add vowel labels

    if vowel != None:
        for line,row in vowelcsv.iterrows():
            pl.text(vowelcsv[F2][line]+0.1,
                    vowelcsv[F1][line],
                    vowelcsv[vowel][line],
                    horizontalalignment = 'left',
                    size = 14, # Edit for larger plots!
                    color = 'black',
                   # weight = 'semibold' # Uncomment for larger plots!
                   )

    pl.set_title(title)
    plt.show()

    return pl



def barkify (data, formants):
    # For each formant listed, make a copy of the column prefixed with z
    for formant in formants:
        for ch in formant:
            if ch.isnumeric():
                num = ch
        formantchar = (formant.split(num)[0])
        name = str(formant).replace(formantchar,'z')
        # Convert each value from Hz to Bark
        data[name] = 26.81/ (1+ 1960/data[formant]) - 0.53
    # Return the dataframe with the changes
    return data

def Lobify (data, group, formants):
    zscore = lambda x: (x - x.mean()) / x.std()
    for formant in formants:
        name = str("zsc_" + formant)
        col = data.groupby([group])[formant].transform(zscore)
        data.insert(len(data.columns), name, col)
    return data

barked_data = barkify(phones_analysis, ["F1", "F2"])

lobified_data = Lobify(barked_data,
                 group = "file",
                 formants = ["z1", "z2"]
                )
vowelplot(lobified_data,
          F1 = "zsc_z1",
          F2 = "zsc_z2",
          color = "cs_next",
          unit = "Lobanov normalized from Bark",
          logscale = False)
vowelplot(barked_data,
          F1 = "z1",
          F2 = "z2",
          color = "cs_next",
          unit = "Bark",
          logscale = False)
lobified_data.to_csv('/content/drive/MyDrive/dataverse_files/barked_data.csv')

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('/content/drive/MyDrive/dataverse_files/barked_data.csv')
df_cs = df[df['cs_next'] == True]
df_no_cs = df[df['cs_next'] == False]

df_cs_1 = df_cs.iloc[:300,:]
df_cs_2 = df_cs.iloc[300:,:]

df_no_cs_1 = df_no_cs.iloc[:300,:]
df_no_cs_2 = df_no_cs.iloc[300:,:]

df = pd.concat([df_cs_1, df_no_cs_1])
df2 = pd.concat([df_cs_2, df_no_cs_2])

print(len(df_cs))
print(len(df_no_cs))
# Preprocess the data
# Choose relevant columns and convert them to tensors
label_encoder = LabelEncoder()
df['phone'] = label_encoder.fit_transform(df['phone'])
features = df[['zsc_z1', 'zsc_z2', 'phone']].values
labels = df['cs_next'].astype(int).values

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)

# Convert data to PyTorch tensors
train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))
val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))

# Create DataLoader
batch_size = 32
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)

# Define the neural network
class VowelFormantPredictor(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(VowelFormantPredictor, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Initialize the model, loss function, and optimizer
input_size = 3  # F1, F2, stress, phone
hidden_size = 64  # Adjust as needed
output_size = 2  # Binary classification (before or after code-switch)
model = VowelFormantPredictor(input_size, hidden_size, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 11

for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()

        # Extract the first 4 columns (F1, F2, stress, phone)
        inputs = batch_X

        outputs = model(inputs)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    average_loss = total_loss / len(train_loader)

    # Validation
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for batch_X_val, batch_y_val in val_loader:
            val_inputs = batch_X_val
            val_outputs = model(val_inputs)
            val_loss += criterion(val_outputs, batch_y_val).item()

    average_val_loss = val_loss / len(val_loader)

    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Val Loss: {average_val_loss:.4f}')

label_encoder = LabelEncoder()
df2['phone'] = label_encoder.fit_transform(df2['phone'])
features2 = df2[['zsc_z1', 'zsc_z2','phone']].values
labels2 = df2['cs_next'].astype(int).values

# Split the data into training, validation, and test sets
# The test_size parameter determines the percentage of data allocated to the test set
X_train, X_temp, y_train, y_temp = train_test_split(features2, labels2, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Convert the data to consistent data types
X_test = np.array(X_test, dtype=np.float32)
y_test = np.array(y_test, dtype=np.long)

# Now, X_train, y_train is your training set
# X_val, y_val is your validation set
# X_test, y_test is your test set

test_data = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

# Test the model on the test set
model.eval()

predictions = []
ground_truth = []

with torch.no_grad():
    for batch_X_test, batch_y_test in test_loader:
        test_inputs = batch_X_test
        test_outputs = model(test_inputs)

        _, predicted = torch.max(test_outputs, 1)

        predictions.extend(predicted.cpu().numpy())
        ground_truth.extend(batch_y_test.cpu().numpy())

# Convert predictions and ground truth to NumPy arrays
predictions = np.array(predictions)
ground_truth = np.array(ground_truth)

# Calculate overall accuracy
correct_predictions = (predictions == ground_truth).sum()
total_samples = len(ground_truth)
accuracy = correct_predictions / total_samples * 100

print(f'Overall Accuracy: {accuracy:.2f}%')
# Now, you can inspect each prediction and ground truth label
for i in range(len(predictions)):
    print(f"Example {i + 1}: Prediction={predictions[i]}, Ground Truth={ground_truth[i]}")

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('/content/drive/MyDrive/dataverse_files/speech_rate.csv')


df_cs = df[df['cs_next'] == True]
df_no_cs = df[df['cs_next'] == False].sample(600)

df_cs_1 = df_cs.iloc[:300,:]
df_cs_2 = df_cs.iloc[300:,:]

df_no_cs_1 = df_no_cs.iloc[:300,:]
df_no_cs_2 = df_no_cs.iloc[425:,:]

df = pd.concat([df_cs_1, df_no_cs_1])
df2 = pd.concat([df_cs_2, df_no_cs_2])

print(len(df_cs))
print(len(df_no_cs))
# Preprocess the data
# Choose relevant columns and convert them to tensors
label_encoder = LabelEncoder()
# df['phone'] = label_encoder.fit_transform(df['phone'])
features = df[['syll/dur']].values
labels = df['cs_next'].astype(int).values

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)

# Convert data to PyTorch tensors
train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))
val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))

# Create DataLoader
batch_size = 32
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)

# Define the neural network
class VowelFormantPredictor(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(VowelFormantPredictor, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Initialize the model, loss function, and optimizer
input_size = 1  # F1, F2, stress, phone
hidden_size = 64  # Adjust as needed
output_size = 2  # Binary classification (before or after code-switch)
model = VowelFormantPredictor(input_size, hidden_size, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 11

for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()

        # Extract the first 4 columns (F1, F2, stress, phone)
        inputs = batch_X

        outputs = model(inputs)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    average_loss = total_loss / len(train_loader)

    # Validation
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for batch_X_val, batch_y_val in val_loader:
            val_inputs = batch_X_val
            val_outputs = model(val_inputs)
            val_loss += criterion(val_outputs, batch_y_val).item()

    average_val_loss = val_loss / len(val_loader)

    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Val Loss: {average_val_loss:.4f}')

label_encoder = LabelEncoder()
# df2['phone'] = label_encoder.fit_transform(df2['phone'])
features2 = df2[['syll/dur']].values
labels2 = df2['cs_next'].astype(int).values

# Split the data into training, validation, and test sets
# The test_size parameter determines the percentage of data allocated to the test set
X_train, X_temp, y_train, y_temp = train_test_split(features2, labels2, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Convert the data to consistent data types
X_test = np.array(X_test, dtype=np.float32)
y_test = np.array(y_test, dtype=np.long)

# Now, X_train, y_train is your training set
# X_val, y_val is your validation set
# X_test, y_test is your test set

test_data = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

# Test the model on the test set
model.eval()

predictions = []
ground_truth = []

with torch.no_grad():
    for batch_X_test, batch_y_test in test_loader:
        test_inputs = batch_X_test
        test_outputs = model(test_inputs)

        _, predicted = torch.max(test_outputs, 1)

        predictions.extend(predicted.cpu().numpy())
        ground_truth.extend(batch_y_test.cpu().numpy())

# Convert predictions and ground truth to NumPy arrays
predictions = np.array(predictions)
ground_truth = np.array(ground_truth)

# Calculate overall accuracy
correct_predictions = (predictions == ground_truth).sum()
total_samples = len(ground_truth)
accuracy = correct_predictions / total_samples * 100

print(f'Overall Accuracy: {accuracy:.2f}%')
# Now, you can inspect each prediction and ground truth label
for i in range(len(predictions)):
    print(f"Example {i + 1}: Prediction={predictions[i]}, Ground Truth={ground_truth[i]}")

df = pd.read_csv('/content/drive/MyDrive/dataverse_files/barked_data.csv')

df['mfcc'] = ''
def mfcc_praat(x, start, end):
        f0min, f0max = 75, 300
        sound = parselmouth.Sound(x) # read the sound
        sound = sound.extract_part(from_time = start, to_time = end)
        mfcc_object = sound.to_mfcc(number_of_coefficients=12)
        mfcc = mfcc_object.to_array()
        return mfcc

for index, row in df.iterrows():
  print(str(index) + "/" + str(len(df)))
  sound = "/content/drive/MyDrive/dataverse_files/english/" + row['file'] + ".wav"
  start = row['phone_start']
  end = row['phone_end']
  mfcc = mfcc_praat(sound, start, end)
  df.at[index,'mfcc'] = mfcc

print(df)

dataset = df[['phone', 'mfcc', 'cs_next']]
print(dataset)
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np

# Assuming df is your DataFrame
dataset['phone'] = dataset['phone'].astype(str)  # Ensure 'phone' column is of string type

# Flatten the nested lists in the 'mfcc' column
dataset['mfcc'] = dataset['mfcc'].apply(lambda x: [item for sublist in x for item in sublist])

# Drop rows with missing values
dataset= dataset.dropna()

# Convert data types to ensure compatibility
dataset['phone'] = pd.to_numeric(dataset['phone'], errors='coerce')
dataset['mfcc'] = dataset['mfcc'].apply(lambda x: np.asarray(x, dtype=np.float32))
dataset['cs_next'] = dataset['cs_next'].astype(bool)

# Get the maximum sequence length
max_len = max(len(x) for x in dataset['mfcc'].values)

# Manually pad sequences to the maximum length
mfcc_list_padded = [np.pad(x, (0, max_len - len(x)), 'constant') for x in df['mfcc'].values]
X = torch.tensor(mfcc_list_padded, dtype=torch.float32)

# Reshape X to have an additional dimension
X = X.unsqueeze(2)

# Convert y to PyTorch tensor
y = torch.tensor(df['cs_next'].values, dtype=torch.float32).reshape(-1, 1)

# Define the model
model = nn.Sequential(
    nn.LSTM(input_size=1, hidden_size=12, batch_first=True),
    nn.ReLU(),
    nn.Linear(12, 8),
    nn.ReLU(),
    nn.Linear(8, 1),
    nn.Sigmoid()
)
print(model)

loss_fn = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

n_epochs = 100
batch_size = 10

for epoch in range(n_epochs):
    for i in range(0, len(X), batch_size):
        Xbatch = X[i:i+batch_size]
        y_pred, _ = model(Xbatch)
        ybatch = y[i:i+batch_size]
        loss = loss_fn(y_pred[:, -1, :], ybatch)  # Select only the last time step output
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        print(f'Finished epoch {epoch}, batch {i//batch_size}, loss {loss.item()}')

# Compute accuracy (no_grad is optional)
with torch.no_grad():
    y_pred, _ = model(X)
accuracy = ((y_pred[:, -1, :] > 0.5) == y).float().mean()
print(f"Accuracy {accuracy.item()}")

# Make class predictions with the model
predictions = (y_pred[:, -1, :] > 0.5).int()
for i in range(5):
    print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i].item(), y[i].item()))

import math
import pandas as pd
import parselmouth

from glob import glob
from parselmouth.praat import call

words_df = pd.read_csv('/content/drive/MyDrive/dataverse_files/word_data.csv')
def speech_rate(filename, start, end):
    silencedb = -25
    mindip = 2
    minpause = 0.3
    sound = parselmouth.Sound(filename)
    sound = sound.extract_part(from_time = start, to_time = end)
    originaldur = sound.get_total_duration()
    intensity = sound.to_intensity(50)
    start = call(intensity, "Get time from frame number", 1)
    nframes = call(intensity, "Get number of frames")
    end = call(intensity, "Get time from frame number", nframes)
    min_intensity = call(intensity, "Get minimum", 0, 0, "Parabolic")
    max_intensity = call(intensity, "Get maximum", 0, 0, "Parabolic")

    # get .99 quantile to get maximum (without influence of non-speech sound bursts)
    max_99_intensity = call(intensity, "Get quantile", 0, 0, 0.99)

    # estimate Intensity threshold
    threshold = max_99_intensity + silencedb
    threshold2 = max_intensity - max_99_intensity
    threshold3 = silencedb - threshold2
    if threshold < min_intensity:
        threshold = min_intensity

    # get pauses (silences) and speakingtime
    textgrid = call(intensity, "To TextGrid (silences)", threshold3, minpause, 0.1, "silent", "sounding")
    silencetier = call(textgrid, "Extract tier", 1)
    silencetable = call(silencetier, "Down to TableOfReal", "sounding")
    npauses = call(silencetable, "Get number of rows")
    speakingtot = 0
    for ipause in range(npauses):
        pause = ipause + 1
        beginsound = call(silencetable, "Get value", pause, 1)
        endsound = call(silencetable, "Get value", pause, 2)
        speakingdur = endsound - beginsound
        speakingtot += speakingdur

    intensity_matrix = call(intensity, "Down to Matrix")
    # sndintid = sound_from_intensity_matrix
    sound_from_intensity_matrix = call(intensity_matrix, "To Sound (slice)", 1)
    # use total duration, not end time, to find out duration of intdur (intensity_duration)
    # in order to allow nonzero starting times.
    intensity_duration = call(sound_from_intensity_matrix, "Get total duration")
    intensity_max = call(sound_from_intensity_matrix, "Get maximum", 0, 0, "Parabolic")
    point_process = call(sound_from_intensity_matrix, "To PointProcess (extrema)", "Left", "yes", "no", "Sinc70")
    # estimate peak positions (all peaks)
    numpeaks = call(point_process, "Get number of points")
    t = [call(point_process, "Get time from index", i + 1) for i in range(numpeaks)]

    # fill array with intensity values
    timepeaks = []
    peakcount = 0
    intensities = []
    for i in range(numpeaks):
        value = call(sound_from_intensity_matrix, "Get value at time", t[i], "Cubic")
        if value > threshold:
            peakcount += 1
            intensities.append(value)
            timepeaks.append(t[i])

    # fill array with valid peaks: only intensity values if preceding
    # dip in intensity is greater than mindip
    validpeakcount = 0
    if len(timepeaks) == 0:
      timepeaks.append(0)
    currenttime = timepeaks[0]
    if len(intensities) == 0:
      intensities.append(0)
    currentint = intensities[0]
    validtime = []

    for p in range(peakcount - 1):
        following = p + 1
        followingtime = timepeaks[p + 1]
        dip = call(intensity, "Get minimum", currenttime, timepeaks[p + 1], "None")
        diffint = abs(currentint - dip)
        if diffint > mindip:
            validpeakcount += 1
            validtime.append(timepeaks[p])
        currenttime = timepeaks[following]
        currentint = call(intensity, "Get value at time", timepeaks[following], "Cubic")

    # Look for only voiced parts
    pitch = sound.to_pitch_ac(0.02, 30, 4, False, 0.03, 0.25, 0.01, 0.35, 0.25, 450)
    voicedcount = 0
    voicedpeak = []

    for time in range(validpeakcount):
        querytime = validtime[time]
        whichinterval = call(textgrid, "Get interval at time", 1, querytime)
        whichlabel = call(textgrid, "Get label of interval", 1, whichinterval)
        value = pitch.get_value_at_time(querytime)
        if not math.isnan(value):
            if whichlabel == "sounding":
                voicedcount += 1
                voicedpeak.append(validtime[time])

    # calculate time correction due to shift in time for Sound object versus
    # intensity object
    timecorrection = originaldur / intensity_duration

    # Insert voiced peaks in TextGrid
    call(textgrid, "Insert point tier", 1, "syllables")
    for i in range(len(voicedpeak)):
        position = (voicedpeak[i] * timecorrection)
        call(textgrid, "Insert point", 1, position, "")

    # return results
    speakingrate = voicedcount / originaldur
    articulationrate = voicedcount / speakingtot
    npause = npauses - 1
    if voicedcount > 0:
      asd = speakingtot / voicedcount
    else:
      asd = None
    speechrate_dictionary = {'soundname':filename,
                             'nsyll':voicedcount,
                             'npause': npause,
                             'dur(s)':originaldur,
                             'phonationtime(s)':intensity_duration,
                             'speechrate(nsyll / dur)': speakingrate,
                             "articulation rate(nsyll / phonationtime)":articulationrate,
                             "ASD(speakingtime / nsyll)":asd}
    return speechrate_dictionary

for index, row in words_df.iterrows():
  print(str(index) + "/" + str(len(words_df)))
  sound = "/content/drive/MyDrive/dataverse_files/english/" + row['file'] + ".wav"
  start = row['word_start']
  end = row['word_end']
  try:
    result = speech_rate(sound, start, end)
  except:
    result = None
  print(result)